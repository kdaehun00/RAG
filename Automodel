
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from typing import Dict
import torch
import numpy as np
from transformers import AutoModel, AutoTokenizer
from sentence_transformers.util import cos_sim

# For retrieval you need to pass this prompt. Please find our more in our blog post.
def transform_query(query: str) -> str:
    """ For retrieval, add the prompt for query (not for documents).
    """
    return f'Represent this sentence for searching relevant passages: {query}'

# The model works really well with cls pooling (default) but also with mean pooling.
def pooling(outputs: torch.Tensor, inputs: Dict,  strategy: str = 'cls') -> np.ndarray:
    if strategy == 'cls':
        outputs = outputs[:, 0]
    elif strategy == 'mean':
        outputs = torch.sum(
            outputs * inputs["attention_mask"][:, :, None], dim=1) / torch.sum(inputs["attention_mask"], dim=1, keepdim=True)
    else:
        raise NotImplementedError
    return outputs.detach().cpu().numpy()

model_id = 'mixedbread-ai/mxbai-embed-large-v1'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModel.from_pretrained(model_id).to('cpu')

# Streamlit UI 구성
st.title("Automodel 기반 QA 시스템")
st.write("PDF 문서를 로드하여 검색합니다.")

# PDF 파일 경로 (사용자가 로드한 파일)
pdf_filepath = "KB.pdf"

# 검색창만 표시
query = st.text_input("질문을 입력하세요:")

if query:
    try:
        # PDF 로드
        pdf_loader = PyPDFLoader(pdf_filepath)
        with st.spinner("PDF 문서 처리 중..."):
            docs = pdf_loader.load()  # PDF 전체 텍스트 로드

            # 텍스트 분할기 설정
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=400,
                chunk_overlap=200
            )

            inputs = tokenizer(docs, padding=True, return_tensors='pt')
            for k, v in inputs.items():
                inputs[k] = v.cuda()
            outputs = model(**inputs).last_hidden_state
            embeddings = pooling(outputs, inputs, 'cls')

            similarities = cos_sim(embeddings[0], embeddings[1:])
            print('similarities:', similarities)


    except Exception as e:
        st.error(f"PDF 처리 중 오류 발생: {e}")
